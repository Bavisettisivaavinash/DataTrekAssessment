{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53aa30b2-35fe-4779-b12e-b120891515ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bavisettisivaavinash/hadoopworker/spark-3.5.1-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed5f6a-ab1c-4ac9-bb10-c679c0382762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadExcel\").getOrCreate()\n",
    "\n",
    "# Read the Excel file using pandas\n",
    "excel_df1 = pd.read_excel('Customer_Product_modified.xlsx')\n",
    "excel_df2 = pd.read_excel('Customer_Transaction_history_modified.xlsx')\n",
    "excel_df3 = pd.read_excel('Customer_Channel_Activity_modified.xlsx')\n",
    "excel_df5 = pd.read_excel('Product_Lookup_modified.xlsx')\n",
    "\n",
    "\n",
    "spark_df1 = spark.createDataFrame(excel_df1)\n",
    "spark_df2 = spark.createDataFrame(excel_df2)\n",
    "spark_df3 = spark.createDataFrame(excel_df3)\n",
    "spark_df5 = spark.createDataFrame(excel_df5)\n",
    "\n",
    "spark_df4 = spark.read.csv('customer_demographic_modified.xlsx - demographic.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfa1d7-1b15-4af9-8f4f-08210bf1a462",
   "metadata": {},
   "source": [
    "changed datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a9f4db-8329-4082-9c4d-3b17ed03a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "spark_df1 = spark_df1.withColumn('Start date', to_date(col('start date'))).withColumn('End date', to_date(col('End date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddca5354-8a55-4a73-95d4-9f421fd29f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CSID: long (nullable = true)\n",
      " |-- Prod code: long (nullable = true)\n",
      " |-- Start date: date (nullable = true)\n",
      " |-- End date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d510cfc-5e61-4f53-be52-e69b41f409af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+----------+\n",
      "|      CSID|Prod code|Start date|  End date|\n",
      "+----------+---------+----------+----------+\n",
      "|2345450601|     7001|2021-12-17|4399-12-31|\n",
      "|2345450602|     3001|1996-10-29|4399-12-31|\n",
      "|2345450603|     8004|2021-06-01|4399-12-31|\n",
      "|2345450604|     7001|2020-08-12|4399-12-31|\n",
      "|2345450605|     5004|2010-05-05|4399-12-31|\n",
      "|2345450606|     8004|2017-06-19|4399-12-31|\n",
      "|2345450607|     1004|2014-09-13|4399-12-31|\n",
      "|2345450608|     2003|2023-06-10|4399-12-31|\n",
      "|2345450609|     5002|2011-09-21|4399-12-31|\n",
      "|2345450610|     2002|2020-03-09|4399-12-31|\n",
      "|2345450611|     1006|1978-02-06|4399-12-31|\n",
      "|2345450612|     3004|2020-05-20|4399-12-31|\n",
      "|2345450613|     5001|2004-10-01|4399-12-31|\n",
      "|2345450614|     8005|2022-12-07|4399-12-31|\n",
      "|2345450615|     2005|2024-01-25|4399-12-31|\n",
      "|2345450616|     2005|2022-12-15|4399-12-31|\n",
      "|2345450617|     6003|2004-01-23|4399-12-31|\n",
      "|2345450618|     3005|2009-03-04|4399-12-31|\n",
      "|2345450619|     5002|2011-05-15|4399-12-31|\n",
      "|2345450620|     2002|2017-10-22|4399-12-31|\n",
      "+----------+---------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80717db-720c-488d-ae93-5b4f674733af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CSID: long (nullable = true)\n",
      " |-- Trans_type: string (nullable = true)\n",
      " |-- Merchant_Category: string (nullable = true)\n",
      " |-- Merchant_Code: long (nullable = true)\n",
      " |-- Trans Amount: double (nullable = true)\n",
      " |-- Trans Date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ae09c6-5007-4557-8bb0-2c795f6fa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df2 = spark_df2.withColumn('Trans Date', to_date(col('Trans Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02ce78a-51c5-4925-aded-0752ed69944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:13:40 WARN TaskSetManager: Stage 3 contains a task of very large size (3278 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+-------------+------------+----------+\n",
      "|      CSID|          Trans_type|Merchant_Category|Merchant_Code|Trans Amount|Trans Date|\n",
      "+----------+--------------------+-----------------+-------------+------------+----------+\n",
      "|2345450601|Regular Payment Card|    Miscellaneous|         8567|      714.44|2022-02-02|\n",
      "|2345450602|Regular Payment Card|    Digital Goods|         8568|      434.93|1997-07-06|\n",
      "|2345450603|Regular Payment Card| Food & Beverages|         8569|     1463.98|2022-08-24|\n",
      "|2345450604|Regular Payment Card|         Airlines|         8570|     1367.74|2020-10-18|\n",
      "|2345450605|          Debit Card|             fuel|         8571|      149.43|2010-07-15|\n",
      "+----------+--------------------+-----------------+-------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:13:44 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 5): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e42dbc6-8a84-4c03-a2d9-569385d948d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:13:44 WARN TaskSetManager: Stage 4 contains a task of very large size (1622 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+---------------+\n",
      "|      CSID|  Channel|       ActivityDate|   ActivityType|\n",
      "+----------+---------+-------------------+---------------+\n",
      "|2345450601|Telephone|2022-05-28 00:00:00|Account Opening|\n",
      "|2345450611|      App|1987-05-21 00:00:00|Account Opening|\n",
      "|2345450616|   Branch|2024-05-01 00:00:00|Account Opening|\n",
      "|2345450635|      App|2022-01-17 00:00:00|Account Opening|\n",
      "|2345450637|      App|2023-11-18 00:00:00|Account Opening|\n",
      "+----------+---------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:13:48 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4 (TID 6): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4dd2989-d0dd-40b0-944b-a3fa0ad275e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CSID: long (nullable = true)\n",
      " |-- Channel: string (nullable = true)\n",
      " |-- ActivityDate: timestamp (nullable = true)\n",
      " |-- ActivityType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1d5b92-cd22-4130-b7c7-409f5c82ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "spark_df3 = spark_df3.withColumn('ActivityDate', date_format(to_date(col('ActivityDate')), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e63bed-80cd-4381-ae28-aeac3a816dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df3 = spark_df3.withColumn('ActivityDate', to_date(col('ActivityDate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37027e1-0fbb-47ce-b233-39387601321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "|      CSID|Income|dependants|Gender|Birth date|Relationship Start date|Marital_Status|Zip_Code|\n",
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "|2345450601|200000|         3|     ?|  3/8/1976|               8/6/2017|      Divorced|CH42 0HS|\n",
      "|2345450602|100000|         2|     ?|28-11-1963|               8/3/1983|      Divorced|ME20 6PR|\n",
      "|2345450603|280000|         2|     ?|  5/6/1979|               1/9/2008|       Married|OX17 1EQ|\n",
      "|2345450604|300000|         1|     ?|  9/5/1999|             10/12/2020|      Divorced| RH7 6LT|\n",
      "|2345450605|150000|         1|  Male|31-03-2004|               6/3/2011|       Married| NW2 7RJ|\n",
      "|2345450606|100000|         3|     ?|18-02-1987|             17-12-2002|        Single| TQ1 3PZ|\n",
      "|2345450607|130000|         2|     M| 9/12/1961|             27-11-1983|      Divorced| LU6 3NP|\n",
      "|2345450608|130000|         1|Female|23-02-1978|             15-11-1994|       Married|DA14 6NY|\n",
      "|2345450609|235000|         3|     F|  2/2/1967|             29-08-1996|        Single|RH16 2QL|\n",
      "|2345450610|300000|         3|Female|13-03-1988|             25-04-2011|       Married|PO37 6EG|\n",
      "|2345450611|200000|         2|     ?| 10/9/1975|               1/8/1997|      Divorced|BB11 4HW|\n",
      "|2345450612|250000|         2|Female| 12/8/1992|             18-01-2000|        Single| TN4 9JU|\n",
      "|2345450613|200000|         0|Female|17-09-1988|             26-06-2017|       Widowed|SA67 8QB|\n",
      "|2345450614|500000|         2|  Male| 11/9/1982|               3/5/2003|       Married| TA1 4XD|\n",
      "|2345450615|120000|         0|  Male|13-01-1991|             18-09-2022|        Single|WF12 8AZ|\n",
      "|2345450616|150000|         1|     ?|24-12-1963|             30-12-1987|       Married| WA8 9UF|\n",
      "|2345450617|150000|         0|Female| 10/2/1989|             14-12-2011|        Single|  B2 4DU|\n",
      "|2345450618|300000|         0|     ?|11/11/1992|             28-02-2013|       Married| LA4 6DA|\n",
      "|2345450619|150000|         2|     ?|27-10-1992|             18-08-2004|       Married| SN3 6DR|\n",
      "|2345450620|250000|         3|     ?|27-10-1992|             26-03-1994|       Married|ME15 8BW|\n",
      "+----------+------+----------+------+----------+-----------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22041013-810c-4883-b29e-457a1840af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CSID: long (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- dependants: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Birth date: string (nullable = true)\n",
      " |-- Relationship Start date: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Zip_Code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4467f2f5-9d68-4cb6-898d-f3d0eed7b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_date(date_str):\n",
    "    for fmt in ('%d-%m-%Y', '%m/%d/%Y'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "convert_date_udf = udf(convert_date, StringType())\n",
    "\n",
    "spark_df4 = spark_df4.withColumn('Birth date', convert_date_udf(col('Birth date')))\n",
    "spark_df4 = spark_df4.withColumn('Relationship Start date', convert_date_udf(col('Relationship Start date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4dd470f-e8c5-4fb7-85a0-f88fee766388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "spark_df4 = spark_df4.withColumn('Gender', when(col('Gender') == '?', 'unidentified').otherwise(col('Gender')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b19c6d06-52c5-4058-9ddb-1cd80e17812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "spark_df4 = spark_df4.withColumn('Zip_Code', regexp_replace(col('Zip_Code'), ' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a38149-ca5e-4ad6-bde0-1391b44753db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df4 = spark_df4.withColumn('Birth date', to_date(col('Birth date'))).withColumn('Relationship Start date', to_date(col('Relationship Start date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0850a07c-a730-4ad5-97fa-4f9754751588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Prod code: long (nullable = true)\n",
      " |-- Prod cls: string (nullable = true)\n",
      " |-- Prod name: string (nullable = true)\n",
      " |-- Start Date: timestamp (nullable = true)\n",
      " |-- End Date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f540575d-d897-424d-be45-1ff232dffd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+-------------------+-------------------+\n",
      "|Prod code|Prod cls|          Prod name|         Start Date|           End Date|\n",
      "+---------+--------+-------------------+-------------------+-------------------+\n",
      "|     1001|   Loans|          Home loan|1980-01-01 00:00:00|2007-06-13 00:00:00|\n",
      "|     1002|   Loans|      Personal Loan|1980-01-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1003|   Loans|           car loan|2001-06-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1004|   Loans|       student loan|2007-06-13 00:00:00|4599-12-31 00:00:00|\n",
      "|     1005|   Loans|          gold loan|2010-05-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1006|   Loans|    automobile loan|1975-06-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1007|   Loans|      buisness loan|2000-05-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1008|   Loans|small business loan|1980-01-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     1009|   Loans|    home loan extra|2016-12-04 00:00:00|4599-12-31 00:00:00|\n",
      "|     1010|   Loans| personal loan plus|2010-03-11 00:00:00|4599-12-31 00:00:00|\n",
      "|     1011|   Loans|          home loan|2007-06-14 00:00:00|4599-12-31 00:00:00|\n",
      "|     2001|   Cards|             Travel|2000-05-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     2002|   Cards|            rewards|2008-12-01 00:00:00|4599-12-31 00:00:00|\n",
      "|     2003|   Cards|           business|2018-12-25 00:00:00|4599-12-31 00:00:00|\n",
      "|     2004|   Cards|     credit builder|1975-06-01 00:00:00|2005-02-10 00:00:00|\n",
      "|     2005|   Cards|           purchase|2015-03-10 00:00:00|4599-12-31 00:00:00|\n",
      "|     2006|   Cards|        travel plus|2005-03-10 00:00:00|4599-12-31 00:00:00|\n",
      "|     2007|   Cards|     credit builder|2005-03-10 00:00:00|4599-12-31 00:00:00|\n",
      "|     3001| Savings|          Max Saver|1995-06-30 00:00:00|4599-12-31 00:00:00|\n",
      "|     3002| Savings|          Tax Saver|1995-06-30 00:00:00|4599-12-31 00:00:00|\n",
      "+---------+--------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1add0a0a-2635-4f58-9eff-3eda37e8845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df5 = spark_df5.withColumn('Start Date', date_format(to_date(col('Start Date')), 'yyyy-MM-dd')).withColumn('End Date', date_format(to_date(col('End Date')), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f500123-0195-412b-b1ed-994bdbf9f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df5 = spark_df5.withColumn('Start Date', to_date(col('Start Date'))).withColumn('End Date', to_date(col('End Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fec7ad7-d2c4-4510-a30b-bddcff71702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df4.write.csv('Barclays/DataTrekAssessment', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b25710b1-b638-4010-94ff-216e2f534149",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df5.write.csv('Barclays/DataTrekAssessment/5', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38c8645c-6594-4e8c-9329-ae36a3796317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:13:59 WARN TaskSetManager: Stage 10 contains a task of very large size (1622 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df3.write.csv('Barclays/DataTrekAssessment/3', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9957166e-b288-4399-bafc-96626c04c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df1.write.csv('Barclays/DataTrekAssessment/1', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "280d9be8-d08d-4782-85bd-2ec6206c726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:14:00 WARN TaskSetManager: Stage 12 contains a task of very large size (3278 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark_df2.write.csv('Barclays/DataTrekAssessment/2', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "489e45de-af82-4d3c-99a6-305774c6b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF1 = spark_df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bec72c66-1ba1-4cec-92aa-6f0e36766c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 15:14:01 WARN TaskSetManager: Stage 14 contains a task of very large size (3278 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/11 15:14:05 WARN TaskSetManager: Stage 15 contains a task of very large size (1622 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandasDF2 = spark_df2.toPandas()\n",
    "pandasDF3 = spark_df3.toPandas()\n",
    "pandasDF4 = spark_df4.toPandas()\n",
    "pandasDF5 = spark_df5.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34136629-3baa-4a01-a2a1-3123348596f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF1.to_csv('Customer_Product_modified.csv', header=True, index=False)\n",
    "pandasDF2.to_csv('Customer_Transaction_history_modified.csv', header=True, index=False)\n",
    "pandasDF3.to_csv('Customer_Channel_Activity_modified.csv', header=True, index=False)\n",
    "pandasDF4.to_csv('customer_demographic_modified.csv', header=True, index=False)\n",
    "pandasDF5.to_csv('Product_Lookup_modified.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
